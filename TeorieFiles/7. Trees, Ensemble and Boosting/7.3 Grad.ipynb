{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метод градиентного спуска\n",
    "## Минимизируй это\n",
    "\n",
    "Рассмотрим способ минимизации функции при помощи градиентного спуска. Начнём с определения градиента: \n",
    "\n",
    "$$\\nabla f = \\frac{\\partial f}{\\partial x_1}\\vec{e_1} + \\frac{\\partial f}{\\partial x_2}\\vec{e_2} + ... + \\frac{\\partial f}{\\partial x_n}\\vec{e_n}$$\n",
    "\n",
    "Это вектор, компоненты которого являются частными производными функции. Важная особенность градиента заключается в том, что он показывает направление наискорейшего роста функции. Тогда антиградиент $-\\nabla f$ показывает направление, в котором функция убывает. Это очень удобно в задачах минимизации. В рамках машинного обучения это может использоваться для поиска оптимальных параметров $\\theta$, например, в задаче регрессии, когда их количество слишком велико. \n",
    "\n",
    "Рассмотрим простой синтетический одномерный пример только лишь для наглядной визуализации. Найдём минимум функции $f = (\\theta - 3)^2$. Выбрав некоторое начальное приближение $\\theta_0$ и двигаясь в направлении антиградиента $\\theta_1 = \\theta_0 - \\alpha\\nabla f$ с некоторым шагом $\\alpha$ будем получать всё меньшие и меньшие значения целевой функции:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "theta = np.linspace(-2, 8)\n",
    "y = f(theta)\n",
    "\n",
    "plt.plot(theta, y);\n",
    "plt.xlim(-2, 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве начального приближения $\\theta_0$ выберем точку $\\theta_0 = 0$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "theta = np.linspace(-2, 8)\n",
    "y = f(theta)\n",
    "\n",
    "theta0 = 0\n",
    "y0 = f(theta0)\n",
    "\n",
    "plt.plot(theta, y);\n",
    "plt.scatter(theta0, y0, c='r');\n",
    "plt.xlim(-2, 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Антиградиент $-\\nabla f = 2(3 - \\theta)$ в точке $\\theta_0$ будет равен $2(3 - \\theta_0) = 2(3 - 0) = 6$. Выберем шаг $\\alpha = 0.1$ и сделаем несколько итераций:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return (x - 3) ** 2\n",
    "\n",
    "def grad(x):\n",
    "    return 2*(x - 3)\n",
    "\n",
    "alpha = 0.2\n",
    "\n",
    "theta = np.linspace(-2, 8)\n",
    "y = f(theta)\n",
    "\n",
    "\n",
    "theta_min = [0]\n",
    "for i in range(10):\n",
    "    theta_current = theta_min[-1]\n",
    "    theta_i = theta_current - alpha * grad(theta_current)\n",
    "    \n",
    "    theta_min.append(theta_i)\n",
    "\n",
    "\n",
    "plt.plot(theta, y);\n",
    "plt.scatter(theta_min, f(np.array(theta_min)), c=range(len(theta_min))[::-1], cmap='autumn');\n",
    "plt.xlim(-2, 8);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проблему локальных минимумов и выбора начального приближения мы не будем рассматривать здесь."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
