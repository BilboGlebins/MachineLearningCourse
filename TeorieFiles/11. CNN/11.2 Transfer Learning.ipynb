{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Большинство эффективных моделей, особенно это касается победителей ImageNet, обладают одной общей и неприятной особенностью - количество их параметров измеряется десятками и сотнями миллионов. Это много. И обучение сети с такой архитектурой - долгая и ресурсоёмкая задача. Однако, это ещё не повод не использовать такие сети в своих задачах, поскольку существует Transfer Learning. \n",
    "\n",
    "Идея, которая лежит за этим названием, следующая. Допустим, кто-то обучил Inception v3 и получил хороший результат на ImageNet. Допустим этот кто-то настолько хороший человек, что поделился своей моделькой со всеми желающими. Но эта моделька работает только для классов, представленных в ImageNet, а мы хотим написать сеть, которая отличает крокодилов от аллигаторов. \n",
    "\n",
    "А теперь вспоминаем, какие признаки выделяют фильтры первых свёрточных слоёв. Это очень простые признаки - вертикальные и горизонтальные границы, диагональные линии, яркие точки на тёмном фоне и так далее. Следующие слои выделяют чуть более сложные, однако по-прежнему очень общие, признаки - элементарные формы. Почему бы нам не оставить эти слои в покое и не учить только некоторое количество последних слоёв? Это сильно уменьшит количество параметров для обучения.\n",
    "\n",
    "Резюмируем: идея Transfer Learning заключается в том, чтобы взять хорошую модель, оставить базовые фильтры и переучить фильтры высоких порядков под свою задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning в Keras\n",
    "\n",
    "## Использование предобученной модели\n",
    "\n",
    "Готовые модели лежат в модуле `keras.applications`. Для того, чтобы посмотреть доступные модели можете написать `from keras.applications import ` и нажать на Tab, для выбора варианта автозаполнения. Возьмём для эксперимента что-нибудь полегче, например, [MobileNet](https://habr.com/ru/post/352804/). И сразу же посмотрим её архитектуру (та ещё простыня)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "model = MobileNet()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы подгрузить сразу обученную на ImageNet модель, при создании модели в качестве параметров нужно передать `weights='imagenet'`. Помимо этого можно указать параметр `include_top=False`, чтобы не подгружать последние полносвязные слои модели, используемые в качестве дискриминаторов, то есть, сопоставляющие признакам, которые выявили свёртки, класс, в данном случае один из тысячи классов ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь расположим её слои по номерам (заодно посмотрим, какие слои ушли с параметром include_top) и выберем, какие заморозить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#\tName\t\t\t\tTrainable Parameters\n",
      "0\tinput_10              \t0\n",
      "1\tconv1_pad             \t0\n",
      "2\tconv1                 \t864\n",
      "3\tconv1_bn              \t128\n",
      "4\tconv1_relu            \t0\n",
      "5\tconv_dw_1             \t288\n",
      "6\tconv_dw_1_bn          \t128\n",
      "7\tconv_dw_1_relu        \t0\n",
      "8\tconv_pw_1             \t2048\n",
      "9\tconv_pw_1_bn          \t256\n",
      "10\tconv_pw_1_relu        \t0\n",
      "11\tconv_pad_2            \t0\n",
      "12\tconv_dw_2             \t576\n",
      "13\tconv_dw_2_bn          \t256\n",
      "14\tconv_dw_2_relu        \t0\n",
      "15\tconv_pw_2             \t8192\n",
      "16\tconv_pw_2_bn          \t512\n",
      "17\tconv_pw_2_relu        \t0\n",
      "18\tconv_dw_3             \t1152\n",
      "19\tconv_dw_3_bn          \t512\n",
      "20\tconv_dw_3_relu        \t0\n",
      "21\tconv_pw_3             \t16384\n",
      "22\tconv_pw_3_bn          \t512\n",
      "23\tconv_pw_3_relu        \t0\n",
      "24\tconv_pad_4            \t0\n",
      "25\tconv_dw_4             \t1152\n",
      "26\tconv_dw_4_bn          \t512\n",
      "27\tconv_dw_4_relu        \t0\n",
      "28\tconv_pw_4             \t32768\n",
      "29\tconv_pw_4_bn          \t1024\n",
      "30\tconv_pw_4_relu        \t0\n",
      "31\tconv_dw_5             \t2304\n",
      "32\tconv_dw_5_bn          \t1024\n",
      "33\tconv_dw_5_relu        \t0\n",
      "34\tconv_pw_5             \t65536\n",
      "35\tconv_pw_5_bn          \t1024\n",
      "36\tconv_pw_5_relu        \t0\n",
      "37\tconv_pad_6            \t0\n",
      "38\tconv_dw_6             \t2304\n",
      "39\tconv_dw_6_bn          \t1024\n",
      "40\tconv_dw_6_relu        \t0\n",
      "41\tconv_pw_6             \t131072\n",
      "42\tconv_pw_6_bn          \t2048\n",
      "43\tconv_pw_6_relu        \t0\n",
      "44\tconv_dw_7             \t4608\n",
      "45\tconv_dw_7_bn          \t2048\n",
      "46\tconv_dw_7_relu        \t0\n",
      "47\tconv_pw_7             \t262144\n",
      "48\tconv_pw_7_bn          \t2048\n",
      "49\tconv_pw_7_relu        \t0\n",
      "50\tconv_dw_8             \t4608\n",
      "51\tconv_dw_8_bn          \t2048\n",
      "52\tconv_dw_8_relu        \t0\n",
      "53\tconv_pw_8             \t262144\n",
      "54\tconv_pw_8_bn          \t2048\n",
      "55\tconv_pw_8_relu        \t0\n",
      "56\tconv_dw_9             \t4608\n",
      "57\tconv_dw_9_bn          \t2048\n",
      "58\tconv_dw_9_relu        \t0\n",
      "59\tconv_pw_9             \t262144\n",
      "60\tconv_pw_9_bn          \t2048\n",
      "61\tconv_pw_9_relu        \t0\n",
      "62\tconv_dw_10            \t4608\n",
      "63\tconv_dw_10_bn         \t2048\n",
      "64\tconv_dw_10_relu       \t0\n",
      "65\tconv_pw_10            \t262144\n",
      "66\tconv_pw_10_bn         \t2048\n",
      "67\tconv_pw_10_relu       \t0\n",
      "68\tconv_dw_11            \t4608\n",
      "69\tconv_dw_11_bn         \t2048\n",
      "70\tconv_dw_11_relu       \t0\n",
      "71\tconv_pw_11            \t262144\n",
      "72\tconv_pw_11_bn         \t2048\n",
      "73\tconv_pw_11_relu       \t0\n",
      "74\tconv_pad_12           \t0\n",
      "75\tconv_dw_12            \t4608\n",
      "76\tconv_dw_12_bn         \t2048\n",
      "77\tconv_dw_12_relu       \t0\n",
      "78\tconv_pw_12            \t524288\n",
      "79\tconv_pw_12_bn         \t4096\n",
      "80\tconv_pw_12_relu       \t0\n",
      "81\tconv_dw_13            \t9216\n",
      "82\tconv_dw_13_bn         \t4096\n",
      "83\tconv_dw_13_relu       \t0\n",
      "84\tconv_pw_13            \t1048576\n",
      "85\tconv_pw_13_bn         \t4096\n",
      "86\tconv_pw_13_relu       \t0\n",
      "87\tbatch_normalization_21\t4096\n",
      "88\tflatten_8             \t0\n",
      "89\tdense_21              \t40141600\n",
      "90\tbatch_normalization_22\t3200\n",
      "91\tdense_22              \t320400\n",
      "92\tbatch_normalization_23\t1600\n",
      "93\tdense_23              \t40100\n",
      "94\tbatch_normalization_24\t400\n",
      "95\tdropout_8             \t0\n",
      "96\tdense_24              \t202\n"
     ]
    }
   ],
   "source": [
    "print('#\\tName\\t\\t\\t\\tTrainable Parameters')\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(i, '%-22s' % layer.name, layer.count_params(), sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем считать, что хватит первых 8 свёрточных слоёв. Последний слой, получается, номер 55. Заморозим до него. Однако, [в Керасе есть одна неприятная вещь](http://blog.datumbox.com/the-batch-normalization-layer-of-keras-is-broken/), касающаяся реализации BatchNorm слоёв. \n",
    "\n",
    "Подробнее о баге читайте в статейке, я лишь озвучу главное следствие: **во время Transfer Learning нельзя замораживать BatchNorm слои.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Датасет\n",
    "\n",
    "В качестве учебного датасета будем использовать [этот](https://github.com/kendemu/woman-man-recog). Будем определять по фотографии, кто перед нами - парень или девушка. И это хороший выбор, потому, что в ImageNet нет таких классов.\n",
    "\n",
    "Для начала датасет нужно скачать. [Инструкция как пользоваться Google Drive в Colab и ссылки на датасеты](http://iostream.pw/how_to_drive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U -q PyDrive\n",
    "\n",
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# import pickle\n",
    "\n",
    "# # Authenticate and create the PyDrive client.\n",
    "# auth.authenticate_user()\n",
    "# gauth = GoogleAuth()\n",
    "# gauth.credentials = GoogleCredentials.get_application_default()\n",
    "# drive = GoogleDrive(gauth)\n",
    "\n",
    "\n",
    "\n",
    "# id = '1nvStwFCTvsS_8wilNK1VTbk_x2G7hiLj'\n",
    "\n",
    "# downloaded = drive.CreateFile({'id': id}) \n",
    "# downloaded.GetContentFile('data.pickle') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию, которая разбивает датасет на обучающую и валидационную выборку и возвращает их в формате, подобному тому, что мы видели в MNIST: `(x_train, y_train), (x_val, y_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "def load_data(train_percentage=0.8):\n",
    "    dataset = pickle.load(open('man_or_woman.pickle', 'rb'))\n",
    "    men_train, women_train = np.random.rand(dataset['men'].shape[0]) < train_percentage, np.random.rand(dataset['women'].shape[0]) < train_percentage\n",
    "    men_val, women_val = ~men_train, ~women_train\n",
    "\n",
    "    x_train = np.concatenate([dataset['men'][men_train], dataset['women'][women_train]], axis=0)\n",
    "    x_val = np.concatenate([dataset['men'][men_val], dataset['women'][women_val]], axis=0)\n",
    "\n",
    "    y_train, y_val = np.zeros(shape=(x_train.shape[0], 1), dtype=bool), np.zeros(shape=(x_val.shape[0], 1), dtype=bool)\n",
    "    y_train[:np.sum(men_train)] = 1\n",
    "    y_val[:np.sum(men_val)] = 1\n",
    "    y_train, y_val = np.hstack([y_train, ~y_train]), np.hstack([y_val, ~y_val])\n",
    "    \n",
    "    shuffled_order = np.random.choice(np.arange(x_train.shape[0]), x_train.shape[0], replace=False)\n",
    "    \n",
    "    return (x_train[shuffled_order], y_train[shuffled_order]), (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pyzip import PyZip\n",
    "\n",
    "dogz = PyZip().from_file('catordogs/dogs.zip')\n",
    "catz = PyZip().from_file('catordogs/cats.zip')\n",
    "\n",
    "cats = np.array([plt.imread(io.BytesIO(catz[fname]), format='jpg') for fname in catz])\n",
    "dogs = np.array([plt.imread(io.BytesIO(dogz[fname]), format='jpg') for fname in dogz])\n",
    "\n",
    "def load_data():\n",
    "    dogs_train, cats_train = np.random.rand(dogs.shape[0]) < 0.9, np.random.rand(cats.shape[0]) < 0.9\n",
    "    dogs_val, cats_val = ~dogs_train, ~cats_train\n",
    "    \n",
    "    x_train = np.concatenate([dogs[dogs_train], cats[cats_train]], axis=0)\n",
    "    x_val = np.concatenate([dogs[dogs_val], cats[cats_val]], axis=0)\n",
    "    \n",
    "    y_train, y_val = np.zeros(shape=(x_train.shape[0], 1), dtype=bool), np.zeros(shape=(x_val.shape[0], 1), dtype=bool)\n",
    "    y_train[:np.sum(dogs_train)] = 1\n",
    "    y_val[:np.sum(dogs_val)] = 1\n",
    "    y_train, y_val = np.hstack([y_train, ~y_train]), np.hstack([y_val, ~y_val])\n",
    "    \n",
    "    shuffled_order = np.random.choice(np.arange(x_train.shape[0]), x_train.shape[0], replace=False)\n",
    "    return (x_train[shuffled_order], y_train[shuffled_order]), (x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1002 samples, validate on 404 samples\n",
      "Epoch 1/40\n",
      "1002/1002 [==============================] - 13s 13ms/step - loss: 0.6737 - binary_accuracy: 0.7226 - val_loss: 0.2144 - val_binary_accuracy: 0.9183\n",
      "Epoch 2/40\n",
      "1002/1002 [==============================] - 5s 5ms/step - loss: 0.2383 - binary_accuracy: 0.9032 - val_loss: 0.3265 - val_binary_accuracy: 0.8787\n",
      "Epoch 3/40\n",
      "1002/1002 [==============================] - 5s 5ms/step - loss: 0.1740 - binary_accuracy: 0.9381 - val_loss: 0.1718 - val_binary_accuracy: 0.9233\n",
      "Epoch 4/40\n",
      "1002/1002 [==============================] - 5s 5ms/step - loss: 0.1571 - binary_accuracy: 0.9431 - val_loss: 0.5708 - val_binary_accuracy: 0.8762\n",
      "Epoch 5/40\n",
      "1002/1002 [==============================] - 5s 5ms/step - loss: 0.0393 - binary_accuracy: 0.9850 - val_loss: 0.5429 - val_binary_accuracy: 0.8886\n",
      "Epoch 6/40\n",
      "1002/1002 [==============================] - 5s 5ms/step - loss: 0.0264 - binary_accuracy: 0.9860 - val_loss: 0.3562 - val_binary_accuracy: 0.9134\n",
      "Epoch 7/40\n",
      " 768/1002 [=====================>........] - ETA: 1s - loss: 0.0255 - binary_accuracy: 0.9935"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c0ba0a002b5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.6.4/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.applications import MobileNet\n",
    "from keras import Model\n",
    "from keras.layers import Convolution2D, Reshape, Dropout, Softmax, Flatten, Dense, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.metrics import binary_accuracy\n",
    "\n",
    "# Подгружаем модель с весами\n",
    "model = MobileNet(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Записываем её выход\n",
    "x = model.output\n",
    "# И, посольку он является выходом свёртки, сделаем его плоским\n",
    "x = Flatten()(x)\n",
    "\n",
    "# Напишем дискриминатор в виде автоэнкодера (несколько полносвязных слоёв с понижением количества элементов):\n",
    "x = Dense(400, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(100, activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "\n",
    "x = Dense(20, activation='relu')(x)\n",
    "x = Dropout(0.1)(x)\n",
    "\n",
    "# И выходной слой на два класса:\n",
    "out = Dense(2, activation='softmax')(x)\n",
    "\n",
    "# После чего соберём нашу новую модель, указав её вход и новый выход:\n",
    "model = Model(inputs=model.input, outputs=out)\n",
    "\n",
    "# А теперь сам Transfer Learning. Чтобы заморозить слои достаточно просто выставить их атрибут .trainable в False\n",
    "for N in range(0):\n",
    "    layer = model.layers[N]\n",
    "    is_not_batchnorm = not isinstance(model.layers[3], BatchNormalization)\n",
    "    layer.trainable = False if is_not_batchnorm else True  # Не забываем про то, что нельзя замораживать batchnorm\n",
    "\n",
    "# Learning rate возьмём ниже, чем обычно - мы ведь доучиваем. Всё остальное - как обычно. Ииии обучаем.\n",
    "optimizer = Adam(lr=0.0001, amsgrad=True)  \n",
    "loss = binary_crossentropy\n",
    "metrics = [binary_accuracy]\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = load_data(0.7)\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          validation_data=(x_val, y_val), \n",
    "          epochs=40, \n",
    "          batch_size=32, \n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И наконец проверим, что напредсказывала наша обученная модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatches: 35; Accuracy = 91.13924050632912%\n",
      "(35,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2400x1600 with 24 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "gender = {0: 'M', 1: 'F'}\n",
    "\n",
    "pred = model.predict(x_val)\n",
    "predicted_class = np.argmax(pred, axis=1) \n",
    "\n",
    "match = predicted_class == np.argmax(y_val, axis=1)\n",
    "mismatch = ~match\n",
    "print(f'Total mismatches: {mismatch.sum()}; Accuracy = {match.mean() * 100}%')\n",
    "\n",
    "x = x_val[mismatch]\n",
    "y = np.argmax(y_val, axis=1)[mismatch]\n",
    "p = predicted_class[mismatch]\n",
    "\n",
    "fig, axes = plt.subplots(4, 6, figsize=(24, 16));\n",
    "samples = np.random.choice(np.arange(y.shape[0]), axes.size, replace=False)\n",
    "for i, ax in zip(samples, axes.ravel()):\n",
    "    ax.imshow(x[i], cmap='gray_r')\n",
    "    ax.set_title(f'Predicted as {gender[p[i]]}. True labels is {gender[y[i]]}')\n",
    "    \n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
